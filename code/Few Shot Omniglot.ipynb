{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import errno\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import shutil\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import Module\n",
    "from torch.nn.modules.loss import _assert_no_grad\n",
    "from tqdm import tqdm\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Download the OmniGlot dataset into a torch.Dataset. This will make it easy to batch load examples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Override __getitem__, __len__\n",
    "cache = {}\n",
    "class OmniglotDataset(data.Dataset):\n",
    "    download_urls = [\n",
    "        'https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip',\n",
    "        'https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip'\n",
    "    ]\n",
    "    \n",
    "    #copy the train/test/val splits from the Vinyals paper\n",
    "    split_url = 'https://raw.githubusercontent.com/jakesnell/prototypical-networks/master/data/omniglot/splits/vinyals/'\n",
    "    splits = {\n",
    "        'test': split_url + 'test.txt',\n",
    "        'train': split_url + 'train.txt',\n",
    "        'trainval': split_url + 'trainval.txt',\n",
    "        'val': split_url + 'val.txt',\n",
    "    }\n",
    "    splits_path = os.path.join('splits', 'vinyals')\n",
    "    raw = 'raw'\n",
    "    processed = 'data'\n",
    "\n",
    "    def __init__(self, mode='train', root='../omniglot', download=True):\n",
    "        '''\n",
    "        @@mode: which of the sets to work with\n",
    "        @@root: the directory where the dataset will be stored\n",
    "        @@download: downloads the dataset\n",
    "        '''\n",
    "        super(OmniglotDataset, self).__init__()\n",
    "        self.root = root\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        self.classes = self.get_current_classes(os.path.join(self.root, self.splits_path, mode + '.txt')) #for mode \n",
    "        self.items = self.find_items(os.path.join(self.root, self.processed), self.classes)\n",
    "        self.idx_classes = self.index_classes(self.items)\n",
    "        \n",
    "        #y is a category label\n",
    "        paths, self.y = zip(*[self.get_path_label(pl) for pl in range(len(self))]) \n",
    "        self.x = map(self.load_img, paths, range(len(paths))) #add the actual image to the Dataset object after rotation/resize\n",
    "        self.x = list(self.x) #cast this to a list of images\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        return x, self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    \n",
    "    def get_current_classes(self, fname):\n",
    "        with open(fname) as f:\n",
    "            classes = f.read().splitlines()\n",
    "        return classes\n",
    "    \n",
    "    def find_items(self, root_dir, classes):\n",
    "        '''\n",
    "        returns a list of tuples with filename, label root, and its rotation\n",
    "        '''\n",
    "        retour = []\n",
    "        #each image can have 4 rotations of the same image\n",
    "        rots = ['/rot000', '/rot090', '/rot180', '/rot270']\n",
    "        for (root, dirs, files) in os.walk(root_dir):\n",
    "            for f in files:\n",
    "                r = root.split('/')\n",
    "                lr = len(r)\n",
    "                label = r[lr - 2] + \"/\" + r[lr - 1]\n",
    "                for rot in rots:\n",
    "                    if label + rot in classes and (f.endswith(\"png\")):\n",
    "                        retour.extend([(f, label, root, rot)])\n",
    "        print(\"== Dataset: Found %d items \" % len(retour))\n",
    "        return retour\n",
    "\n",
    "    def index_classes(self, items):\n",
    "        idx = {}\n",
    "        for i in items:\n",
    "            #if the image and rotation is not in the map, add it to the map (increasing in len)\n",
    "            if (not i[1] + i[-1] in idx):  \n",
    "                idx[i[1] + i[-1]] = len(idx)\n",
    "        print(\"== Dataset: Found %d classes\" % len(idx))\n",
    "        return idx\n",
    "    \n",
    "    def get_path_label(self, index):\n",
    "        filename = self.items[index][0]\n",
    "        rot = self.items[index][-1]  \n",
    "        img = str.join('/', [self.items[index][2], filename]) + rot\n",
    "        target = self.idx_classes[self.items[index][1] + self.items[index][-1]]\n",
    "        return img, target\n",
    "    \n",
    "    def _already_downloaded(self):\n",
    "        return os.path.exists(os.path.join(self.root, self.processed))\n",
    "    \n",
    "    def download(self):\n",
    "        '''\n",
    "        Create and download the splits .txt\n",
    "        Download raw zip'd omniglot data and unzip it. \n",
    "        '''\n",
    "        import zipfile\n",
    "        from six.moves import urllib\n",
    "\n",
    "        if self._already_downloaded():\n",
    "            return\n",
    "\n",
    "        os.makedirs(os.path.join(self.root, self.splits_path))\n",
    "        os.makedirs(os.path.join(self.root, self.raw))\n",
    "        os.makedirs(os.path.join(self.root, self.processed))\n",
    "\n",
    "        for k, url in self.splits.items():\n",
    "            print('== Downloading ' + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition('/')[-1]\n",
    "            file_path = os.path.join(self.root, self.splits_path, filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(data.read())\n",
    "\n",
    "        for url in self.download_urls:\n",
    "            print('== Downloading ' + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition('/')[2]\n",
    "            file_path = os.path.join(self.root, self.raw, filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(data.read())\n",
    "            orig_root = os.path.join(self.root, self.raw)\n",
    "            print(\"== Unzip from \" + file_path + \" to \" + orig_root)\n",
    "            zip_ref = zipfile.ZipFile(file_path, 'r')\n",
    "            zip_ref.extractall(orig_root)\n",
    "            zip_ref.close()\n",
    "            \n",
    "        file_processed = os.path.join(self.root, self.processed)\n",
    "        for p in ['images_background', 'images_evaluation']:\n",
    "            for f in os.listdir(os.path.join(orig_root, p)):\n",
    "                shutil.move(os.path.join(orig_root, p, f), file_processed)\n",
    "            os.rmdir(os.path.join(orig_root, p))\n",
    "        print(\"Download finished.\")\n",
    "\n",
    "    def load_img(self, path, idx):\n",
    "        path, rot = path.split('/rot')\n",
    "        if path in cache:\n",
    "            x = cache[path]\n",
    "        else:\n",
    "            x = Image.open(path)\n",
    "            cache[path] = x\n",
    "        x = x.rotate(float(rot))   #rotate the image\n",
    "        x = x.resize((28, 28))     #resize the image\n",
    "\n",
    "        shape = 1, x.size[0], x.size[1]\n",
    "        x = np.array(x, np.float32, copy=False)\n",
    "        x = 1.0 - torch.from_numpy(x)\n",
    "        x = x.transpose(0, 1).contiguous().view(shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Dataset: Found 82240 items \n",
      "== Dataset: Found 4112 classes\n"
     ]
    }
   ],
   "source": [
    "x = OmniglotDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampler\n",
    "A generator class for yielding a batch of indices with each training iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler: \n",
    "    '''    \n",
    "    Every iteration of the batch indices returns 'num_support' + 'num_query' samples\n",
    "    for 'classes_per_it' random classes.\n",
    "    \n",
    "    __len__ is the number of iterations in an epoch.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "        '''\n",
    "        @@labels: an iterable containing all the labels for the current dataset\n",
    "        @@classes_per_it: number of random classes per iteration\n",
    "        @@num_samples: number of samples for each iteration for each class (support + query)\n",
    "        @@iterations: number of iterations per epoch\n",
    "        '''\n",
    "        super(Sampler, self).__init__()\n",
    "        self.labels = labels\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples\n",
    "        self.iterations = iterations\n",
    "\n",
    "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "        self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "        self.idxs = range(len(self.labels))\n",
    "        self.label_tens = np.empty((len(self.classes), max(self.counts)), dtype=int) * np.nan\n",
    "        self.label_tens = torch.Tensor(self.label_tens)\n",
    "        self.label_lens = torch.zeros_like(self.classes)\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            label_idx = np.argwhere(self.classes == label)[0, 0]\n",
    "            self.label_tens[label_idx, np.where(np.isnan(self.label_tens[label_idx]))[0][0]] = idx\n",
    "            self.label_lens[label_idx] += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        yield a batch of indexes\n",
    "        '''\n",
    "        sample_iter= self.sample_per_class\n",
    "        class_iter = self.classes_per_it\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            batch_size = sample_iter * class_iter\n",
    "            batch = torch.LongTensor(batch_size)\n",
    "            c_idxs = torch.randperm(len(self.classes))[:class_iter]\n",
    "            for i, c in enumerate(self.classes[c_idxs]):\n",
    "                s = slice(i * sample_iter, (i + 1) * sample_iter)\n",
    "                label_idx = np.argwhere(self.classes == c)[0, 0]\n",
    "                sample_idxs = torch.randperm(self.label_lens[label_idx])[:sample_iter]\n",
    "                batch[s] = self.label_tens[label_idx][sample_idxs]\n",
    "            batch = batch[torch.randperm(len(batch))]   # batch is a slice of the data mapping a file to its class label\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns the number of iterations per epoch\n",
    "        '''\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypical Architecture \n",
    "based off of https://arxiv.org/pdf/1703.05175.pdf which provide a clean alternative to matching networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "    return torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "def prototypical_loss(inputs, target, n_support):\n",
    "    '''\n",
    "    Centroids in embedded space by averaging over the support and then calculates distance of each sample to these.\n",
    "    \n",
    "    Distance used in softmax to calculate log_probability for each query. \n",
    "    @@input: model final layer for a batch \n",
    "    @@target: class labels\n",
    "    @@n_support: number of samples for centroids per class\n",
    "    '''\n",
    "    targs = target.data\n",
    "\n",
    "    def supports(c):\n",
    "        #for a class, choooses n_support examples\n",
    "        return torch.nonzero(targs.eq(int(c)))[:n_support].squeeze() \n",
    "\n",
    "    classes = np.unique(targs)\n",
    "    n_classes = len(classes)\n",
    "    n_query = len(torch.nonzero(targs.eq(int(classes[0])))) - n_support   \n",
    "\n",
    "    embedded_idxs = list(map(supports, classes))\n",
    "\n",
    "    prototypes = torch.stack([inputs[i].mean(0) for i in embedded_idxs]) #calculates the prototypes from the embedded\n",
    "\n",
    "    query_idxs = torch.stack(list(map(lambda c: torch.nonzero(targs.eq(int(c)))[n_support:], classes))).view(-1)\n",
    "    query_points = inputs[query_idx]\n",
    "    dists = euclidean_dist(query_points, prototypes)\n",
    "\n",
    "    log_p_y = F.log_softmax(-dists, dim=1).view(n_classes, n_query, -1) #log probs for the query points\n",
    "    target_idx = torch.arange(0, n_classes).view(n_classes, 1, 1)\n",
    "    target_idx = target_inds.expand(n_classes, n_query, 1).long().long()\n",
    "    target_idx = Variable(target_inds, requires_grad=False) #no need for gradients on backward pass \n",
    "\n",
    "    loss = -log_p_y.gather(2, target_idx).squeeze().view(-1).mean() #mean cross entropy\n",
    "    _, pred = log_p_y.max(2) #the largest probability class is our class prediction\n",
    "\n",
    "    acc = torch.eq(pred, target_idx.squeeze()).float().mean()\n",
    "    return loss, acc\n",
    "\n",
    "class PrototypicalLoss(Module):\n",
    "    def __init__(self, n_support):\n",
    "        super(PrototypicalLoss, self).__init__()\n",
    "        self.n_support = n_support\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        '''\n",
    "        http://pytorch.org/docs/master/_modules/torch/nn/modules/module.html#Module.forward\n",
    "        forward pass at every call\n",
    "        '''\n",
    "        _assert_no_grad(target) #speeds up computation\n",
    "        return prototypical_loss(input, target, self.n_support)\n",
    "\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    '''\n",
    "    2d Convolution, BatchNorm, ReLu, MaxPool\n",
    "    Each block is a 3X3 convolution, batch-normed, relu, followed by 2x2 maxpool\n",
    "    '''\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "class ProtoTypicalNet(nn.Module):\n",
    "    '''\n",
    "    Architecture in the paper for embedding to a vector space for weighted KNN computation\n",
    "    '''\n",
    "    def __init__(self, input=1, hidden=64, output=64):\n",
    "        super(ProtoTypicalNet, self).__init__()\n",
    "        #embedded input examples into a space that can be clustered on\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(input, hidden),\n",
    "            conv_block(hidden, hidden),\n",
    "            conv_block(hidden, hidden),\n",
    "            conv_block(hidden, output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
