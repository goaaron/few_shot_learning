{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import errno\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import shutil\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import Module\n",
    "from torch.nn.modules.loss import _assert_no_grad\n",
    "from tqdm import tqdm\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Download the OmniGlot dataset into a torch.Dataset. This will make it easy to batch load examples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Override __getitem__, __len__\n",
    "cache = {}\n",
    "class OmniglotDataset(data.Dataset):\n",
    "    download_urls = [\n",
    "        'https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip',\n",
    "        'https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip'\n",
    "    ]\n",
    "    \n",
    "    #copy the train/test/val splits from the Vinyals paper\n",
    "    split_url = 'https://raw.githubusercontent.com/jakesnell/prototypical-networks/master/data/omniglot/splits/vinyals/'\n",
    "    splits = {\n",
    "        'test': split_url + 'test.txt',\n",
    "        'train': split_url + 'train.txt',\n",
    "        'trainval': split_url + 'trainval.txt',\n",
    "        'val': split_url + 'val.txt',\n",
    "    }\n",
    "    splits_path = os.path.join('splits', 'vinyals')\n",
    "    raw = 'raw'\n",
    "    processed = 'data'\n",
    "\n",
    "    def __init__(self, mode='train', root='../omniglot', download=True):\n",
    "        '''\n",
    "        @@mode: which of the sets to work with\n",
    "        @@root: the directory where the dataset will be stored\n",
    "        @@download: downloads the dataset\n",
    "        '''\n",
    "        super(OmniglotDataset, self).__init__()\n",
    "        self.root = root\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        self.classes = self.get_current_classes(os.path.join(self.root, self.splits_path, mode + '.txt')) #for mode \n",
    "        self.items = self.find_items(os.path.join(self.root, self.processed), self.classes)\n",
    "        self.idx_classes = self.index_classes(self.items)\n",
    "        \n",
    "        #y is a category label\n",
    "        paths, self.y = zip(*[self.get_path_label(pl) for pl in range(len(self))]) \n",
    "        self.x = map(self.load_img, paths, range(len(paths))) #add the actual image to the Dataset object after rotation/resize\n",
    "        self.x = list(self.x) #cast this to a list of images\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        return x, self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    \n",
    "    def get_current_classes(self, fname):\n",
    "        with open(fname) as f:\n",
    "            classes = f.read().splitlines()\n",
    "        return classes\n",
    "    \n",
    "    def find_items(self, root_dir, classes):\n",
    "        '''\n",
    "        returns a list of tuples with filename, label root, and its rotation\n",
    "        '''\n",
    "        retour = []\n",
    "        #each image can have 4 rotations of the same image\n",
    "        rots = ['/rot000', '/rot090', '/rot180', '/rot270']\n",
    "        for (root, dirs, files) in os.walk(root_dir):\n",
    "            for f in files:\n",
    "                r = root.split('/')\n",
    "                lr = len(r)\n",
    "                label = r[lr - 2] + \"/\" + r[lr - 1]\n",
    "                for rot in rots:\n",
    "                    if label + rot in classes and (f.endswith(\"png\")):\n",
    "                        retour.extend([(f, label, root, rot)])\n",
    "        print(\"== Dataset: Found %d items \" % len(retour))\n",
    "        return retour\n",
    "\n",
    "    def index_classes(self, items):\n",
    "        idx = {}\n",
    "        for i in items:\n",
    "            #if the image and rotation is not in the map, add it to the map (increasing in len)\n",
    "            if (not i[1] + i[-1] in idx):  \n",
    "                idx[i[1] + i[-1]] = len(idx)\n",
    "        print(\"== Dataset: Found %d classes\" % len(idx))\n",
    "        return idx\n",
    "    \n",
    "    def get_path_label(self, index):\n",
    "        filename = self.items[index][0]\n",
    "        rot = self.items[index][-1]  \n",
    "        img = str.join('/', [self.items[index][2], filename]) + rot\n",
    "        target = self.idx_classes[self.items[index][1] + self.items[index][-1]]\n",
    "        return img, target\n",
    "    \n",
    "    def _already_downloaded(self):\n",
    "        return os.path.exists(os.path.join(self.root, self.processed))\n",
    "    \n",
    "    def download(self):\n",
    "        '''\n",
    "        Create and download the splits .txt\n",
    "        Download raw zip'd omniglot data and unzip it. \n",
    "        '''\n",
    "        import zipfile\n",
    "        from six.moves import urllib\n",
    "\n",
    "        if self._already_downloaded():\n",
    "            return\n",
    "\n",
    "        os.makedirs(os.path.join(self.root, self.splits_path))\n",
    "        os.makedirs(os.path.join(self.root, self.raw))\n",
    "        os.makedirs(os.path.join(self.root, self.processed))\n",
    "\n",
    "        for k, url in self.splits.items():\n",
    "            print('== Downloading ' + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition('/')[-1]\n",
    "            file_path = os.path.join(self.root, self.splits_path, filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(data.read())\n",
    "\n",
    "        for url in self.download_urls:\n",
    "            print('== Downloading ' + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition('/')[2]\n",
    "            file_path = os.path.join(self.root, self.raw, filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(data.read())\n",
    "            orig_root = os.path.join(self.root, self.raw)\n",
    "            print(\"== Unzip from \" + file_path + \" to \" + orig_root)\n",
    "            zip_ref = zipfile.ZipFile(file_path, 'r')\n",
    "            zip_ref.extractall(orig_root)\n",
    "            zip_ref.close()\n",
    "            \n",
    "        file_processed = os.path.join(self.root, self.processed)\n",
    "        for p in ['images_background', 'images_evaluation']:\n",
    "            for f in os.listdir(os.path.join(orig_root, p)):\n",
    "                shutil.move(os.path.join(orig_root, p, f), file_processed)\n",
    "            os.rmdir(os.path.join(orig_root, p))\n",
    "        print(\"Download finished.\")\n",
    "\n",
    "    def load_img(self, path, idx):\n",
    "        path, rot = path.split('/rot')\n",
    "        if path in cache:\n",
    "            x = cache[path]\n",
    "        else:\n",
    "            x = Image.open(path)\n",
    "            cache[path] = x\n",
    "        x = x.rotate(float(rot))   #rotate the image\n",
    "        x = x.resize((28, 28))     #resize the image\n",
    "\n",
    "        shape = 1, x.size[0], x.size[1]\n",
    "        x = np.array(x, np.float32, copy=False)\n",
    "        x = 1.0 - torch.from_numpy(x)\n",
    "        x = x.transpose(0, 1).contiguous().view(shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Dataset: Found 82240 items \n",
      "== Dataset: Found 4112 classes\n"
     ]
    }
   ],
   "source": [
    "x = OmniglotDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampler\n",
    "A generator class for yielding a batch of indices with each training iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler: \n",
    "    '''    \n",
    "    Every iteration of the batch indices returns 'num_support' + 'num_query' samples\n",
    "    for 'classes_per_it' random classes.\n",
    "    \n",
    "    __len__ is the number of iterations in an epoch.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "        '''\n",
    "        @@labels: an iterable containing all the labels for the current dataset\n",
    "        @@classes_per_it: number of random classes per iteration\n",
    "        @@num_samples: number of samples for each iteration for each class (support + query)\n",
    "        @@iterations: number of iterations per epoch\n",
    "        '''\n",
    "        super(Sampler, self).__init__()\n",
    "        self.labels = labels\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples\n",
    "        self.iterations = iterations\n",
    "\n",
    "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "        self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "        self.idxs = range(len(self.labels))\n",
    "        self.label_tens = np.empty((len(self.classes), max(self.counts)), dtype=int) * np.nan\n",
    "        self.label_tens = torch.Tensor(self.label_tens)\n",
    "        self.label_lens = torch.zeros_like(self.classes)\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            label_idx = np.argwhere(self.classes == label)[0, 0]\n",
    "            self.label_tens[label_idx, np.where(np.isnan(self.label_tens[label_idx]))[0][0]] = idx\n",
    "            self.label_lens[label_idx] += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        yield a batch of indexes\n",
    "        '''\n",
    "        sample_iter= self.sample_per_class\n",
    "        class_iter = self.classes_per_it\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            batch_size = sample_iter * class_iter\n",
    "            batch = torch.LongTensor(batch_size)\n",
    "            c_idxs = torch.randperm(len(self.classes))[:class_iter]\n",
    "            for i, c in enumerate(self.classes[c_idxs]):\n",
    "                s = slice(i * sample_iter, (i + 1) * sample_iter)\n",
    "                label_idx = np.argwhere(self.classes == c)[0, 0]\n",
    "                sample_idxs = torch.randperm(self.label_lens[label_idx])[:sample_iter]\n",
    "                batch[s] = self.label_tens[label_idx][sample_idxs]\n",
    "            batch = batch[torch.randperm(len(batch))]   # batch is a slice of the data mapping a file to its class label\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns the number of iterations per epoch\n",
    "        '''\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypical Architecture \n",
    "based off of https://arxiv.org/pdf/1703.05175.pdf which provide a clean alternative to matching networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
